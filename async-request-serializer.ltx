\documentclass[a4paper, 11pt, onecolumn]{article}
\usepackage{graphicx}
\usepackage{geometry}
	\geometry{
	left=20mm,
	right=20mm,
	top=30mm,
      bottom=30mm
	}
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
  frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=false,
  breakatwhitespace=true,
  tabsize=3
}
\begin{document}

\title{\textsf{\textbf{A Java Library for Conditional Request Serialization of Asynchronous Requests in Parallel}}}
\author{\textsf{Arun Yadav}\\
\textsf{Samsung Research Institute - Bangalore}\\
\texttt{arun.y@samsung.com}}
\date{}
\maketitle
	
\begin{abstract}
This paper describes the design and implementation of a Java library for serializing asynchronous requests by some request criteria, yet parallely processing requests which are independent (or not having same request criteria). This is generally required in scenarios where requests are arriving asynchronously and can be processed parrallely with a condition that request with certain criteria (e.g. request userId) to be processed in chronological order. The measured performance shows good improvement over single threaded processing.
\end{abstract}

\section{Introduction}
Most of the time we use queue based communication if we wanted asynchronous request processing. However cases where messages are being received in asynchronous manner yet we want to serialize/order/or maintain happen-before relationship of the request processing by some criteria say, all requests with same userid to be processed in serial order, we can't simply use queue, because the queue listeners/consumers (if more than one on a single queue) will pick requests independent of other requets and processing will happen parallel. In such situation the happen before relation among requests under same key will be lost. This paper discuss the design and implementation of a Java library that addreses above use case.


\section{Design}

Asynchronous requests can easily be processed in parallel given there is no inter-dependency among arriving request. A simple work queue with multiple listeners (consumers) design can be utilized. Next free listener can pick-up the request and start execution in parallel however, in scenarios where processing needs to maintain happen-before relationship among set of arriving requests with same request key, we need a controlled excution of request. 
The design proposed here takes care of serializing set of requests where happen-before relation is to be maintained, yet parallelly executing other requests where happen-before relatation need not be maintained. The design also takes care of not engulfing the system (by not creating too many processing thread) when number of requests arrives at rate greater than rate at which application can process requests. The library can scale veritcally by increasing the number of processing cores of a given machine.

Below are core desing elements
\begin{itemize}
\item A Worker Thread \textbf{(WT)} with internal/local in-memory work queue.
\item A fixed size Worker Thread Pool \textbf{(WTP)} of \textbf{WTs} is maintained while system is loaded. The default behaviour is to create as many \textbf{WTs} as available CPUs to JVM.
\item A \textbf{WTP} will also maintains a Request Key \textbf{(RK)} to \textbf{WT} mapping \textbf{(RK-WT-M)} for allocating and de-allocating \textbf{WT} against \textbf{RK}.
\item A Single Listener Thread \textbf{(SLT)} to receive request and submitting the same to allocated \textbf{WT}
\end{itemize}

\includegraphics[]{sync-processing-of-async-req-fig-1}


The above four elements co-ordinate among themshelf to meet the system requirement in a following way.

\begin{enumerate}
\item A \textbf{SLT} upon receiving a request \textbf{(R)} along with associated \textbf{RK}, gets a \textbf{WT} from \textbf{WTP}. This call blocks if all \textbf{WT} in \textbf{WTP} is already assigned to some \textbf{RK}.
\item Upon receiving \textbf{WT}, \textbf{SLT} attempts to assign this \textbf{R} to received \textbf{WT}.
\item \textbf{SLT} maintains an attempt count and attempt delay in case assignments fails in previous step, to re-assign \textbf{R} by getting new \textbf{WT} from \textbf{WTP}. The assignment can fail in case \textbf{WT} deactivated iteself (de-allocated against mapped \textbf{RK}) after it was given to \textbf{SLT} by \textbf{WTP}.
\item Upon receiving assignment request from \textbf{SLT}, the \textbf{WT} first gets a locks for local work queue and its state variable. Upon receiving lock, \textbf{WT} checks if state is still active (mapped with \textbf{RK}), if yes it adds request into local work queue and release the lock and return to \textbf{SLT} with success. If given \textbf{WT} is not active, it returns failure to \textbf{SLT}, upon which \textbf{SLT} may re-attempt as described in the step above.
\item \textbf{WT} when active (i.e. allocated/mapped against some \textbf{RK}) is always in two states:
	\begin{enumerate}
	\item Either blocking (with timeout) on local work queue for new request
	\item Or processing the request
	\end{enumerate}
\item While in blocking state, \textbf{WT} has configurable timeout, which gives an opportunity to de-allocat itself when no new request is being assigned before timeout since completion of last processing.
\item Upon timing-out while blocking on work queue, \textbf{WT} aquires a lock for local work queue and its state variable. Test again the local work queue to check if something got added after timing out and aquiring lock. 
\item If something is found, then \textbf{WT} goes ahead and process the received request. If not, then \textbf{WT} prepare itself to de-allocated itself aganst mapped \textbf{RK} and release itself to \textbf{WTP}. This step is essential for optimal performance 
of this system. During de-allocation, the \textbf{WT} can't accept new assignment by \textbf{SLT} (\textbf{SLT} will block during that time). And \textbf{WT} sets its state as inactive. Upon de-allocation/releaseing  itself to \textbf{WTP}, \textbf{WT} goes in waiting state (thread.wait()). 
After which its the responsibilty of \textbf{WTP} to notify(thread.notify()) this \textbf{WT} while re-allocating to some \textbf{RK} and giving out this \textbf{WT} to \textbf{SLT} in step 1.
\item Because the same \textbf{WT} can be re-used for processing \textbf{R} with other \textbf{RK}s. The more time \textbf{WT} blocks itself against new \textbf{R} in local work queue, the less efficient the overall system becomes. However quickly timing-out and de-allocating
itself can also have adverse effect of re-allocation and reloading the initial context for processing incase request processing has heavy initiallization cost per \textbf{RK}. Client application can however configure this parameter based on average rate of request arrival.
\item The \textbf{WTP} has two functions:
	\begin{enumerate}
	\item Returning free \textbf{WT} from pool and mapping it against \textbf{RK}
	\item Releasing \textbf{WT} into pool and un-mapping it against \textbf{RK}
	\end{enumerate}
\item While returning free \textbf{WT} from pool, \textbf{WTP} aquires a lock for worker thread-request-key-map \textbf{(RK-WT-M)}
\item Checks if some \textbf{WT} is already mapped against given \textbf{RK}. If found return the same and release the lock for \textbf{RK-WT-M}
\item If not found borrow a new \textbf{WT} from pool. If borrowing succeeds map the \textbf{RK} with \textbf{WT} in \textbf{RK-WT-M}. 
\item If borrowing fails, it means pool is exhausted and \textbf{WTP} goes on waiting (thread.wait()) till some WT is released back into pool.
\item When \textbf{WT} determines that there is no new pending request, it calls \textbf{WTP} release() function to release itself. The \textbf{WTP} release  function again aquires a lock for \textbf{RT-WT-M} and deletes its entry against \textbf{RK} and then releases to pool notifing (thread.notify()) to wake up \textbf{SLT} thread waiting on availability for new \textbf{WT} in \textbf{WTP} allocate function described above.
\end{enumerate}


\section{Implementation}

The framework is built on top of \texttt{java.concurrent.util} and \texttt{org.apache.commons.pool} packages. The singleton \texttt{WorkerThreadPool} object (referred as \texttt{WTP} above) maintains a pre-defined (configurable) pool of \\texttt{WorkerThread} objects, I have used \texttt{org.apache.commons.pool.impl.GenericObjectPool} to maintain the same. 
The \texttt{AsyncRequestSerializer} is an entry point into framework, which accepts an instance of \texttt{Work} insterface along with a request key. It returns an instance of \texttt{java.util.concurrent.Future} which client can use to retrieve response of submitted work request. A Java \texttt{java.util.HashMap} is being used to keep the map of \textbf{RK} to \textbf{WT}. The access to this map is synchronized between thread requesting a \textbf{WT} vs. thread returning the \textbf{WT} back into the pool.

\linespread{1.5}
Below is the code fragement of \texttt{WorkerThreadPool}
 
\begin{lstlisting}

public class PoolableWorkerThreadPool<U> {


	private final GenericObjectPool<PoolableWorkerThread<U>> workerThreadPool;
	private final Map<String, PoolableWorkerThread<U>> requestKeyWorkerThreadMap;
	private final Object workerThreadPoolLock = new Object();

	public PoolableWorkerThreadPool(
			final AsyncRequestSerializerConfig asyncRequestSerializerConfig) {
		int availableProcessor = Runtime.getRuntime().availableProcessors();
		Config config = new Config();
		int poolsize = Integer.parseInt(asyncRequestSerializerConfig.workerThreadPoolSize);
		config.maxActive = poolsize <= 0 ? availableProcessor
				: poolsize;
		config.whenExhaustedAction = GenericObjectPool.WHEN_EXHAUSTED_FAIL;
		this.workerThreadPool = new GenericObjectPool<PoolableWorkerThread<U>>(
				new PoolableWorkerThreadFactory<U>(this,
						asyncRequestSerializerConfig), config);
		this.requestKeyWorkerThreadMap = new HashMap<String, PoolableWorkerThread<U>>();
	}

	public PoolableWorkerThread<U> getPoolableWorkerThread(final String requestKey)
			throws Exception {
		long st = System.currentTimeMillis();
		synchronized (workerThreadPoolLock) {
			PoolableWorkerThread<U> poolableWorkerThread = requestKeyWorkerThreadMap
					.get(requestKey);
			if (poolableWorkerThread == null) {
				do {
					try {
						poolableWorkerThread = workerThreadPool.borrowObject();
					} catch (NoSuchElementException e) {
						try {
							workerThreadPoolLock.wait();
						} catch (InterruptedException ie) {
							continue;
						}
					}
				} while (poolableWorkerThread == null);
				requestKeyWorkerThreadMap.put(requestKey, poolableWorkerThread);
				poolableWorkerThread.setCurrentRequestKey(requestKey);
			}
			return poolableWorkerThread;
		}
	}

	public void returnPoolableWorkerThread(PoolableWorkerThread<U> workerThread)
			throws Exception {
		long st = System.currentTimeMillis();
		synchronized (workerThreadPoolLock) {
			requestKeyWorkerThreadMap.remove(workerThread
					.getCurrentRequestKey());
			workerThreadPool.returnObject(workerThread);
			workerThreadPoolLock.notifyAll();
		}
	}
}
\end{lstlisting}


The \texttt{WorkerThread} class once assigned to a \textbf{RK} starts processing incoming \texttt{Work} on a single thread, the single thread allows the system to process work request in serial order per request key. The incoming \texttt{Work} will be pushed into a \texttt{java.util.BlockingQueue} upon which the \texttt{WorkerThread} blocks till timeout. The BlockingQueue implementation takes care of blocking the \textbf{WT} till some new \texttt{Work} is arrived or timeout occurs. In current implementation the timeout is pre-defined configurable which should be assigned value based on general nature of rate of new Work per request key. Given too large value will starve other request key and give too small value may not give the benfit of localization during processing of a given request key. In future we can improve the design to dynamically choose the timeout value by observing the previous rate of arrival for a given request key. The idea of local work queue inside \texttt{WorkerThread} is similar to that of "Actor" model".

The \texttt{WorkerThread} maintains an internal boolean state \texttt{isActive} to indicate if its assigned to some request key or not.
As soon as \texttt{WorkerThread} is returned from a pool, \texttt{isActive} will be marked as true. Till the state {isActive} is set, the \texttt{WorkerThread} will accept new \texttt{Work} into its local work queue. 


Below is the complete code for \texttt{WorkerThread}

\begin{lstlisting}
public class PoolableWorkerThread<U> extends Thread {

	private final PoolableWorkerThreadPool myPool;
	private final AsyncRequestSerializerConfig asyncRequestSerializerConfig;

	private ExecutorService executorService = Executors
			.newSingleThreadExecutor();

	private final BlockingQueue<Future<U>> localRequestQueue = new LinkedBlockingQueue<Future<U>>();
	private final Object localRequestQueueLock = new Object();
	private boolean isActive = false;

	public PoolableWorkerThread(final PoolableWorkerThreadPool myPool, final AsyncRequestSerializerConfig asyncRequestSerializerConfig) {
		this.myPool = myPool;
		this.asyncRequestSerializerConfig = asyncRequestSerializerConfig;
	}

	@Override
	public void run() {
		while (true) {
			try {
				Future<U> request = localRequestQueue.poll(
						Integer.parseInt(asyncRequestSerializerConfig.localRequestQueueTimeOut),
						TimeUnit.MILLISECONDS);
				if (request == null) {
					synchronized (localRequestQueueLock) {
						request = localRequestQueue.poll();
						if (request == null) {
							myPool.returnPoolableWorkerThread(this);
							isActive = false;
							currentRequestKey = null;
							do {
								try {
									/*To awaken, please call activate()*/
									localRequestQueueLock.wait();
									break;//break from wait
								} catch (InterruptedException e) {
									//"Interrupted or spurious wake up, 
									// will check if isActive is set"
								}
							} while (!isActive);
							continue;//continue to look for new work
						}
					}
				}
				/* make sure this executor service is singleThreadExecutor */
				long st = System.currentTimeMillis();
				request.get();
			} catch (Exception e) {
				//"Error while executing local requests"
			}
		}
		executorService.shutdown();
	}

	public Future<U> assign(Work<U> request) {
		synchronized (localRequestQueueLock) {
			if (isActive) {
				Future<U> future = executorService.submit(request);
				localRequestQueue.add(future);
				return future;
			} else {
				return null;
			}
		}
	}

	public void activate() {
		synchronized (localRequestQueueLock) {
			localRequestQueueLock.notify();
			isActive = true;
		}
	}
}
\end{lstlisting}

\section{Conclusion}
The current design guarantees serial processing of all request under same request key, yet parallely processing other  request key. In our system where we have used this framework has improved the application throughput vs. serially processing requests across request key. The current design may however suffers from a starvation problem in scenarios where \textbf{RKs} are arriving at rate higher than the timeout set for \textbf{WT} and too many such request keys arrives at same time, not allowing mapped \textbf{WT} to release itself. This will cause stravation to other less frequent \textbf{RK}. Though we definately need to address this scenario, the present design assumed that no selected \textbf{RK} will engulf the system at much higer rate then rest. And we can keep the timeout small enough so that \textbf{WT} releases itself frequently. As future improvement we can include some ability to yield \textbf{WT} if its being mapped for long time by parking the pending requests in local work queue. 
In addition to incremental improvents, future work on this framework may include adding ability to achieve the same abilty on a distributed enviroment, that will allow horizontal scaling within the framework without externally controlling user synchronization across machine.

\section{References}

\begin{thebibliography}{2}

\bibitem{Lea00}
  Doug Lea,
  "A Java Fork/Join Framework"
  \emph{Proceedings of the ACM 2000 Conference on Java Grande},
  2000.

\bibitem{Lea00}
  Rajesh K. Karmani, Gul Agah,
  "Actor"
  \emph{http://www.cs.ucla.edu/~palsberg/course/cs239/papers/karmani-agha.pdf}.


\end{thebibliography}

\end{document}
